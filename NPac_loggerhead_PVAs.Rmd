---
title: "North Pacific loggerhead PVA"
output: html_notebook
---

Some original model/projection code by Rob Ahrens on 4/18/18 while @ IRC -->
Modified, expanded, and annotated by SLM in July 2018 -->
Population = North Pacific Loggerheads -->
Data source = Sea Turtle Association of Japan (confidential, do not share) -->


```{r}

rm(list=ls())    # clear working space
library(ggplot2) # load plotting packing
library(nlstools)

# setwd("C:/Users/Summer.Martin/SLM documents/SLM-8-Turtle_Population_Models_and_Trends/Turtle PVA and fishery impacts -Ahrens lab")

# Define a color for confidence interval bounds on plot
#col2rgb("gray", alpha=TRUE) # get specs on gray color to use in polygon and tweak transparency
mygray <- rgb(red=190, green=190, blue=190, alpha=200, maxColorValue=255)

# Select from the options below on how to estimate 'r' using the time-series data
fit.option <- 2    # or 2 or 3

# DEFINE plotting FUNCTION to show all sim runs + abundance thresholds
# plot SIMULATION PROJECTION POINTS predicted by EACH SIMULATION RUN at each year in the future to show distribution of outcomes
# shows all simulation runs

# choose rdyn or rstat and whether to add lines for thresholds
plot.simruns <- function(r.sel="rstat", thresh.lines=FALSE) {    
    if(r.sel=="rstat") {
      pN <- pN.rstat 
      g.title <- "Simulation projection with static r"
    }     # 'r' static: runs from simulation with r static for each run
    if(r.sel=="rdyn")  {
      pN <- pN.rdyn 
      g.title <- "Simulation projection with dynamic r"
    }
    #head(as.vector(pN.))            # creates the vector going column by column (i.e., sim 1 yr by yr, then sim 2 yr by yr, etc.)
    rep4plot <- cbind(rep(1:nrow(pN), 
                          times=ncol(pN)), 
                      as.vector(pN))  # Zach's fix to Rob's code (same as my guess below)
    colnames(rep4plot) <- c("x","y")   # x = Year; y = Nests
    x <- rep4plot[,1]
    y <- log(rep4plot[,2])   # can change here to log(Nests) or regular exponential curve of Nests
    df <- data.frame(x = x, 
                     y = y, 
                     d = densCols(x, y, 
                                  colramp = colorRampPalette(rev(rainbow(10, end = 4/6)))))
    
    p <- ggplot(df) +
      geom_point(aes(x, y, col = d), 
                 size = 1) +
      scale_color_identity() +
      theme_bw() +
      labs(x = "Year",y=ylab.txt, title=g.title) +
      theme(axis.title.x = element_text(color = "blue", size = 16)) +
      theme(axis.title.y = element_text(color = "blue", size = 16))
    
    # add mean & 95% CI abundance thresholds as horizontal lines for 50%, 25%, and 12.5% abund.
    thresh.vals50 <- 0.50*quantile(log(CurTotFem), probs=c(0.025, 0.50, 0.975))
    thresh.vals25 <- 0.25*quantile(log(CurTotFem), probs=c(0.025, 0.50, 0.975))
    thresh.vals12 <- 0.125*quantile(log(CurTotFem), probs=c(0.025, 0.50, 0.975))
    
    if(thresh.lines==TRUE) {
      p <- p +  geom_hline(yintercept=thresh.vals50[1], 
                           colour="black", 
                           linetype="dashed") +  # 50% abund thresh lower 95% CI
        geom_hline(yintercept=thresh.vals50[2], 
                   colour="black", 
                   linetype="solid") +  # median
        geom_hline(yintercept=thresh.vals50[3], 
                   colour="black", 
                   linetype="dashed") +    # upper 95% CI
        geom_hline(yintercept=thresh.vals25[1], 
                   colour="black", 
                   linetype="dashed") +  # 25% abund thresh lower 95% CI
        geom_hline(yintercept=thresh.vals25[2], 
                   colour="black", 
                   linetype="solid") +  # median
        geom_hline(yintercept=thresh.vals25[3], 
                   colour="black", 
                   linetype="dashed") +    # upper 95% CI
        geom_hline(yintercept=thresh.vals12[1], 
                   colour="black", 
                   linetype="dashed") +  # 12.5% abund thresh lower 95% CI
        geom_hline(yintercept=thresh.vals12[2], 
                   colour="black", 
                   linetype="solid") +  # median
        geom_hline(yintercept=thresh.vals12[3], 
                   colour="black", 
                   linetype="dashed")    # upper 95% CI
    }
    print(p)
    if(r.sel=="rstat") p.rstat <- p    # save graph images outside of function
    if(r.sel=="rdyn") p.rdyn <- p
}
```

REAL DATA: import North Pacific loggerhead NEST count data from Japan (confidential data, do no distribute)

data are 1993-2016 as used by Loggerhead Tri-National team in early 2018 for decision-making

```{r}
data.type <- "annual_Nests"
#data.type <- "runsum_totalNests"
#data.type <- "runsum_totalFemales"
data <- read.csv("data/Nests_Japan_loggerheads_STAJ_data.csv", header=TRUE)

if(data.type=="annual_Nests") {
  nyrs <- length(data$Year)
  names(data) <- c("Year", "N")  # using N instead of Nests or Females to start streamlining code
  #plot(x=data$Year, y=data$N, type="b", main="Annual Nests")
  plot(x=data$Year, y=log(data$N), type="b", main="Annual Nests")
  Ninit <- data$N[1]
}

if(data.type=="runsum_totalNests") {
  data$N <- NA
  for (y in 3:length(data$Nests)) data$N[y] <- sum(data$Nests[(y-2):y]) # 3 year remigration interval, so use 3 year run sum
  data
  data <- data[-(1:2), -2]
  nyrs <- length(data$Year[!is.na(data$N)])
  #plot(x=data$Year, y=data$N, type="b", main="Total Nests")
  plot(x=data$Year, y=log(data$N), type="b", main="Total Nests")
  Ninit <- data$N[3]
}

if(data.type=="runsum_totalFemales") {
  data$N <- NA
  for (y in 3:length(data$Nests)) data$N[y] <- sum(data$Nests[(y-2):y])  # 3 year remigration interval, so use 3 year run sum
  data$N <- data$N/3     # clutch frequency of 3 nests per female to convert from total nests to total females
  data
  data <- data[-(1:2), -2]
  nyrs <- length(data$Year[!is.na(data$N)])
  #plot(x=data$Year, y=data$N, type="b", main="Total Females")
  plot(x=data$Year, y=log(data$N), type="b", main="Total Females")
  Ninit <- data$N[3]
}
```

POPULATION GROWTH RATE = LONG-TERM TREND
r = pop growth rate in stochastic exponential growth model N(t+1)=N(t)exp(rt))
N(t) = N(0)*exp(r*deltat) ... OR
N(t+1) = N(t)*exp(r)  ... equals
log(N(t+1)) = log(N(t)) + r ... thus,   [alternatively: log(N(t)) = log(N(t-deltat)) + r*deltat ; where deltat is in units of years]
r = log(N(t+1)) - log(N(t))

FITTING OPTION 1: to estimate population growth rate 'r':
take natural log diff between year 2 and 1, 3 and 2, 4 and 3, etc.,
r = log(N(t-1)) - log(N(t))
then take the mean & sd of all those differences
95% CI = rmean +/- 1.96*[rsd/sqrt(n)] using a normal distribution


FITTING OPTION 1: to estimate population growth rate 'r':
take natural log diff between year 2 and 1, 3 and 2, 4 and 3, etc.,
r = log(N(t-1)) - log(N(t))
then take the mean & sd of all those differences
95% CI = rmean +/- 1.96*[rsd/sqrt(n)] using a normal distribution

```{r}

#if (fit.option==1) {
  rvec = log(data$N[2:nyrs]/data$N[1:(nyrs-1)])       # take natural log diff between year 2 and 1, 3 and 2, etc., then take mean of all those
  rmean = mean(rvec)                                  # calc mean of sample values above
  rsd = sd(rvec)                                      # take sd of sample values above
  ci95ErrNorm=qnorm(0.975)*rsd/sqrt(nyrs)        # NORMAL dist: assume normal dist for known pop sd; calc error of CI limits as 1.96*sd/sqrt(n)
  rlwrN=rmean-ci95ErrNorm                        # NORMAL dist
  ruprN=rmean+ci95ErrNorm                        # NORMAL dist
  ci95ErrT=qt(0.975,df=nyrs-1)*rsd/sqrt(nyrs)    # Student's T dist: for working w/ sample SD rather than *exact* pop SD
  rlwrT=rmean-ci95ErrT                           # Student's T dist
  ruprT=rmean+ci95ErrT                           # Student's T dist
  r.stats <- as.data.frame(cbind(rmean, rsd, rlwrN, ruprN, rlwrT, ruprT))  # using Normal or T dist are rea
  r.stats
  rlwr=rlwrN   # chosing Normal CI to move forward
  rupr=ruprN   # choosing Normal CI to move forward
  plot(density(rnorm(10000,rmean,rsd)))             # plot distribution of estimated 'r'
  
  abline(v = c(rlwrN,ruprN), col="red")         # CI limits very similar using normal or t-dist
  abline(v=c(rlwrT,ruprT), col="blue")
  hist(rnorm(10000,rmean,rsd))
  abline(v=c(rlwrN,ruprN), col="red")
  abline(v=c(rlwrT,ruprT), col="blue")

  # plot raw data with estimated 'r' as trend line with CI over the data years
  x.pred <- seq(data$Year[1],data$Year[nyrs],0.1)     # x values to predict over
  y.pred <- log(Ninit)                                # initialize vector to start predicted y for each year using the rmean
  y.pred.upr <- log(Ninit)
  y.pred.lwr <- log(Ninit)
  for (i in 2:length(x.pred)) {
    deltat <- x.pred[i]-x.pred[i-1]
    y.pred[i] <- y.pred[i-1] + rmean*deltat          # log(N(t)) = log(N(t-deltat)) + r*deltat ; where deltat is in years (or fractions of a year)
    y.pred.upr[i] <- y.pred.upr[i-1] + rupr*deltat
    y.pred.lwr[i] <- y.pred.lwr[i-1] + rlwr*deltat
  }
  plot(x=data$Year, y=data$N, type="b")             # plot data as Nests
  lines(x=x.pred, y=exp(y.pred), col="green")

  # plot data as log(Nests) and overlay the predicted log(Nests) using the rmean and 95% confidence limits
  plot(x=data$Year, y=log(data$N), type="b", 
       ylim=c(4.7,12.2), xlab="Year", ylab="log(Nests)")
  lines(x=x.pred, y=y.pred, col="green", lwd=2)
  lines(x=x.pred, y=y.pred.upr, col="gray")
  lines(x=x.pred, y=y.pred.lwr, col="gray")
  polygon(x= c(x.pred[1], x.pred, rev(x.pred)),
          y= c(y.pred.lwr[1], y.pred.upr, rev(y.pred.lwr)),
          col= mygray, lty=0)
  points(x=data$Year, y=log(data$N), type="b", pch=16, cex=1.2)

  # Calculate estimated N=# of nests for the last (most recent) year of data: e.g., 2016 for loggerheads
  Nmean.last=exp(y.pred[length(y.pred)])
  Nlwr.last=exp(y.pred.lwr[length(y.pred)])
  Nupr.last=exp(y.pred.upr[length(y.pred)])
  N.last=as.data.frame(cbind(Nmean.last,Nlwr.last,Nupr.last))
  N.last
  Nsd.last=sqrt(nyrs)*(Nupr.last-Nmean.last)/qnorm(0.975)       # using the CI limit equation:  Nupr.last=Nmean.last + qnorm(0.975)*(Nsd/sqrt(n))
  plot(density(rnorm(10000,Nmean.last,Nsd.last)))              # plot distribution of estimated 'N' for final year of data
#}
```

SLM notes: not sure why the 95% confidence interval spans so narrow a band around the mean in the density/histo plots of 'r'


FITTING OPTION 2: to estimate population growth rate 'r':
fit for r using log-linear model stemming from exponential growth model
log(N(t)) = log(N(t-deltat)) + r*deltat      (y = mx + b)

```{r}
#if (fit.option==2){
  lm.mod=lm(formula= log(N) ~ Year, data=data)
  AIC(lm.mod)
  rmean=lm.mod$coefficients[2]    # mean estimate for slope of line (Year variable)
  rse=round(coef(summary(lm.mod))["Year", "Std. Error"],3)  # extract the SE for slope
  rsd=rse*sqrt(nyrs)
  rvar=rsd^2
  rci=confint(lm.mod)[2,]
  lambda.mean=exp(rmean)
  lambda.ci=exp(rci)
  #plot(density(rnorm(10000,rmean,rse)))
  plot(density(rnorm(10000,rmean,rsd)))
  yr.seq <- seq(data$Year[1],data$Year[nyrs],0.1)
  pred.ls <- predict(lm.mod,list(Year=yr.seq), se.fit=TRUE, interval="confidence", level=0.95) # get Std Error of the prediction & 95% CI
  pred.df <- as.data.frame(cbind(pred.ls$fit, SE=pred.ls$se.fit, Year=yr.seq))       # in unites of log(Nests)

  # plot data as log(Nests) and overlay the predicted log(Nests) using the rmean and 95% confidence limits
  ylab.txt <- ifelse(data.type=="annual_Nests", "Ln(Annual Nests)",
                     ifelse(data.type=="runsum_totalNests", "Ln(Total Nests)",
                            ifelse(data.type=="runsum_totalFemales", "Ln(Total Females)","error")))

  plot(x=data$Year, y=log(data$N), type="b", xlab="Year", ylab=ylab.txt)
  polygon(x= c(pred.df$Year[1], pred.df$Year, rev(pred.df$Year)),
          y= c(pred.df$lwr[1], pred.df$upr, rev(pred.df$lwr)),
          col= mygray, lty=0)
  lines(x=pred.df$Year, y=pred.df$fit, col="blue", lwd=2)  # add mean line of regression slope estimate
  points(x=data$Year, y=log(data$N), type="b", pch=16, cex=1.2) # add points back over CI shading

  # Calculate estimated N=# of nests for the last (most recent) year of data: e.g., 2016 for loggerheads
  pred.len=length(pred.df$fit)
  Nmean.last=exp(pred.df$fit[pred.len])
  Nlwr.last=exp(pred.df$lwr[pred.len])
  Nupr.last=exp(pred.df$upr[pred.len])
  Nse.last=exp(pred.df$SE[pred.len])
  N.last=as.data.frame(cbind(Nmean.last,Nlwr.last,Nupr.last))
  N.last
  log(N.last)
  Nsd.last=(Nupr.last-Nmean.last)/qnorm(0.975)        # using CI upr limit equation:  Nupr.last=Nmean.last + qnorm(0.975)*(Nsd.last)
  log(Nsd.last)
  plot(density(rnorm(10000,Nmean.last,Nsd.last)))     # plot distribution of estimated 'N' for final year of data
  #plot(density(rnorm(10000, exp(pred.df$fit[length(pred.df$fit)]), exp(pred.df$SE[length(pred.df$SE)]))))
  # too narrow using only SE from fit!!

  # Year "N minus 1" or Nm1 = year before final year of data
  # [RELEVANT when data are in Annual Nests or Annual Nesters instead of RunSum]
  m1.row <- which(pred.df$Year==(max(data$Year)-1))
  Nmean.m1=exp(pred.df$fit[m1.row])
  Nlwr.m1=exp(pred.df$lwr[m1.row])
  Nupr.m1=exp(pred.df$upr[m1.row])
  Nse.m1=exp(pred.df$SE[m1.row])
  N.m1=as.data.frame(cbind(Nmean.m1,Nlwr.m1,Nupr.m1))
  N.m1
  log(N.m1)
  Nsd.m1=(Nupr.m1-Nmean.m1)/qnorm(0.975)        # using CI upr limit equation:  Nupr.last=Nmean.last + qnorm(0.975)*(Nsd.last)
  log(Nsd.m1)
  plot(density(rnorm(10000,Nmean.m1,Nsd.m1)))     # plot distribution of estimated 'N' for final year of data

  # Year "N minus 2" or Nm2 = two years before final year of data
  # [RELEVANT when data are in Annual Nests or Annual Nesters instead of RunSum]
  m2.row <- which(pred.df$Year==(max(data$Year)-2))
  Nmean.m2=exp(pred.df$fit[m2.row])
  Nlwr.m2=exp(pred.df$lwr[m2.row])
  Nupr.m2=exp(pred.df$upr[m2.row])
  Nse.m2=exp(pred.df$SE[m2.row])
  N.m2=as.data.frame(cbind(Nmean.m2,Nlwr.m2,Nupr.m2))
  N.m2
  log(N.m2)
  Nsd.m2=(Nupr.m2-Nmean.m2)/qnorm(0.975)        # using CI upr limit equation:  Nupr.last=Nmean.last + qnorm(0.975)*(Nsd.last)
  log(Nsd.m2)
  plot(density(rnorm(10000,Nmean.m2,Nsd.m2)))     # plot distribution of estimated 'N' for final year of data
#}
```


FITTING OPTION 3: fit using nonlinear regression model using same exponential growth model underlying

```{r}
#if(fit.option==3) {   # needs a little revision here
  Year0 <- data$Year[1]                      # first year of data
  data$Ratio_Nt_N0 <- data$N/data$N[1]       # divide Nmbr Nests each year by initial N (first year of data)
  nls.mod <- nls(log(Ratio_Nt_N0) ~ r*(Year-Year0), data = data, start=list(r=0.05))         # starting val doesn't change result, just avoids the warning
  AIC(nls.mod)
  coef(summary(nls.mod))
  rmean=coef(summary(nls.mod))[1]
  rse=coef(summary(nls.mod))[2]
  rsd <- rse*sqrt(nyrs)
  time.seq <-seq(data$Year[1],data$Year[nyrs],0.1)
  pred.ls <- exp(predict(nls.mod,list(Year=time.seq), 
                         se.fit=TRUE, interval="confidence", 
                         level=0.95))*data$N[1]
  # PROBLEM encountered here... predict.nls currently ignores se.fit and interval
  # can't get N w/ std dev for 2016
  
  conf.r <- confint2(object=nls.mod, level=0.95)
  r.lo95 <- conf.r[1]
  r.hi95 <- conf.r[2]
  pred.df <- as.data.frame(cbind(Year=time.seq, N=pred.ls))
  pred.df$lwr  <- pred.df$N[1]*exp(r.lo95*(pred.df$Year-Year0))
  pred.df$upr <- pred.df$N[1]*exp(r.hi95*(pred.df$Year-Year0))
  head(pred.df)
  
  plot(x=data$Year, y=data$N, type="b", xlab="Year", ylab=ylab.txt)
  polygon(x= c(pred.df$Year[1], pred.df$Year, rev(pred.df$Year)),
          y= c(pred.df$lwr[1], pred.df$upr, rev(pred.df$lwr)),
          col= mygray, lty=0)
  lines(x=pred.df$Year, y=pred.df$N, col="blue", lwd=2)  # add mean line of regression slope estimate
  points(x=data$Year, y=data$N, type="b", pch=16, cex=1.2) # add points back over CI shading
  nls.pred.df <- pred.df           # save this df

  rmean
  rse
  rsd
  #plot(density(rnorm(10000,rmean3,rse3)))
  #plot(density(rnorm(10000,rmean3,rsd3)))
#}
```

FOR PROJECTION: Forecast using simulation; calculate quantiles across simulation runs to determine how many runs fall above certain quantiles

```{r}
rmean=rmean
rsd=rsd
nlast=Nmean.last                 #  = mean pop size for last year of observed data (starting point for simulation)
sdnlast=Nsd.last                 #  = sd pop size for that last year of data
yrf=100                          # 100 = years into the future
nsims=10000                        # 50 = number of simulation runs
pN.rstat=matrix(nrow=yrf+1,ncol=nsims)   # +1 for yr=0; for static 'r' projection; predicted N (nests) = matrix with YEARS as rows and SIMULATIONS as columns
pN.rdyn=matrix(nrow=yrf+1,ncol=nsims)    # +1 for yr=0; for dynamic 'r' projection; predicted N (nests) = matrix with YEARS as rows and SIMULATIONS as columns
dim(pN.rstat)
```

FUTURE PROJECTION SIMULATION for PVA -- use either simulated or real data from above
Random draw from dist of r for year 1 of sim AND each subsequent year 2:yrf


```{r}
# FUTURE PROJECTION SIMULATION for PVA -- use *either* simulated or real data from above
# Random draw from dist of r for year 1 of sim *AND* each subsequent year 2:yrf
for(sim in 1:nsims)
{
  N0 <- rnorm(1,nlast,sdnlast)   # for both rdynamic & rstatic approaches; start year 0 of sim with random draw from N
  pN.rdyn[1,sim]= N0
  pN.rstat[1,sim]= N0
  rstat.sim=rnorm(1,rmean,rsd)            # for r dynamic, a single constant 'r' to carry through all future years of ONE simulation run
  
  for(yr in 1:yrf)                   # for each year of sim...
  {
    r <- yr + 1     # adjust row index ... first row is year 0, second row is year 1, etc.
    pN.rdyn[r,sim]=pN.rdyn[r-1,sim]*exp(rnorm(1,rmean,rsd))    # (N from previous year) * exp(draw r from dist)
    pN.rstat[r,sim]=pN.rstat[r-1,sim]*exp(rstat.sim)          # (N from previous year) * exp(constant r for each sim run into future)
  }
}

# view final year of simulation; sim left off on 10000 here if nsim=10000
# look at same simulation run for the two approaches... STATIC r gives WAY HIGHER UNCERTAINTY!!
mean(pN.rstat[ ,10000])
mean(pN.rdyn[ ,10000])

# calculate quantiles & means for simulations w/ DYNAMIC 'r' pulled from dist each year into future
quants.rdyn=t(apply(X=pN.rdyn,MARGIN=1,FUN=quantile,probs=c(0.025,0.5,0.975)))    # median + CI for each year; MARGIN=1 indicates rows for a matrix;
means.rdyn=t(apply(X=pN.rdyn,MARGIN=1,FUN=mean))             # mean for each year; MARGIN=1 is for rows
quants.rdyn.df <- as.data.frame(quants.rdyn)
quants.rdyn.df$Year <- 0:yrf
quants.rdyn.df$Mean <- as.vector(means.rdyn)
quants.rdyn.df <- quants.rdyn.df[ ,c("Year", "Mean", "2.5%", "50%", "97.5%")]  # reorder cols; rows=YEARS, cols=quantiles of sim runs (2.5%, 50%, 97.5%)
#quants.rdyn.df

# calculate quantiles & means for simulations w/ STATIC 'r' pulled ONCE from dist and carried as constant in all future years
quants.rstat=t(apply(X=pN.rstat,MARGIN=1,FUN=quantile,probs=c(0.025,0.5,0.975)))    # median + CI for each year; MARGIN=1 indicates rows for a matrix;
means.rstat=t(apply(X=pN.rstat,MARGIN=1,FUN=mean))             # mean for each year; MARGIN=1 is for rows
quants.rstat.df <- as.data.frame(quants.rstat)
quants.rstat.df$Year <- 0:yrf
quants.rstat.df$Mean <- as.vector(means.rstat)
quants.rstat.df <- quants.rstat.df[ ,c("Year", "Mean", "2.5%", "50%",  "97.5%")]  # reorder cols; rows=YEARS, cols=quantiles of sim runs (2.5%, 50%, 97.5%)
#quants.rstat.df
```

plot SIMULATION QUANTILES = MEDIAN PROJECTION line plus shaded 2.5% and 97.5% lines to show distribution of outcomes

```{r}
for (p in 1:2) {
  ocol=rgb(253,106,2,alpha=40,max=255)
  # 'r' static: plot from simulation: median predicted trajectory w/ 2.5% and 97.5% intervals (at each year, takes median, 2.5% & 97.5% quantiles of sim runs)
  if (p==1){
    matplot(x=0:yrf,
            y=log(quants.rstat.df[,3:5]), 
            type="l",
            lty=c(2,1,2),
            lwd=c(1,2,1),
            col="black", 
            xlab="Year", 
            ylab=ylab.txt)
    polygon(c(0:yrf,yrf:0),
            c(log(quants.rstat.df[,3]), 
              log(rev(quants.rstat.df[,5]))),
            col=ocol,
            border=FALSE)
    title(main="Simulation projection with static r")
  }
  # 'r' dynamic: plot from sim'n: median predicted trajectory w/ 2.5% and 97.5% intervals (at each year, takes median, 2.5% & 97.5% quantiles of sim runs)
  if (p==2) {
    matplot(x=0:yrf,
            y=log(quants.rdyn.df[,3:5]), 
            type="l",
            lty=c(2,1,2),
            lwd=c(1,2,1),
            col="black", 
            xlab="Year", 
            ylab=ylab.txt)
    polygon(c(0:yrf,yrf:0),
            c(log(quants.rdyn.df[,3]),
              log(rev(quants.rdyn.df[,5]))),
            col=ocol,
            border=FALSE)
    title(main="Simulation projection with dynamic r")
  }
}
```



EVALUATION PROBABILITIES OF REACHING ABUNDANCE THRESHOLDS:

For a given year in the future (5, 10, 25, 50, 100)
Calculate probability of reaching abundance thresholds
As the proportion of simulation runs falling below the threshold for that year
PIRO wants 95% CI associated with this probability.... currently do this as bootstrap of simulation runs for each evaluation year

select whether to use the projections that used the 'r dynamic' or 'r static' approach
 
```{r}
#rsel <- "rstat"
rsel <- "rdyn"

# based on selection above, pick the corresponding projection matrix
if(rsel=="rdyn")  pN.sel <- pN.rdyn
if(rsel=="rstat") pN.sel <- pN.rstat

# clutch frequency and remigration interval numbers factor in below if need to convert Annual Nests or Total Nests to Total Females
clutch.freq <- 3   # 3 nests per female in a nesting year
remig <- 3         # 3 year remigration interval = 3 year run sum period

# calculate "current" abundance using 3 year run sum of Annual Nests or Annual Nesters
# can simply use N0 from simulation if data type is Total Females (already factors in run sum total earlier in model)
N0 <- pN.sel[1,]   #  first row of sim matrix (same for rdyn & rstat) is Year 0 draw of N in 2016 from dist (final data year for loggers)
Nm1 <- rnorm(10000,Nmean.m1,Nsd.m1)    # 10,000 sim runs pulling random draw from "Final data year minus 1" = 2015 for loggers
Nm2 <- rnorm(10000,Nmean.m2,Nsd.m2)    # 10,000 sim runs pulling random draw from "Final data year minus 2" = 2014 for loggers

# calculate Current TOTAL FEMALES from last data year (best estimate for current abundance for baseline)
# vector with length nsim -- different starting point of Current TOTAL FEMALES for each sim
if(data.type=="annual_Nests") CurTotFem <- (N0 + Nm1 + Nm2)/3        # if data are Annual Nests, 3-yr run sum divided by 3 nests/female
if(data.type=="runsum_totalNests") CurTotFem  <- N0/3                # if data are of TOTAL Nests (data as 3-yr Nest runsums), divide by 3 nests/female
if(data.type=="runsum_totalFemales") CurTotFem <- N0                 # if projections are of TOTAL Females (data as 3-yr Female runsums), just take final
length(CurTotFem)                              # should be number of sims

# Plot sim runs with abundance thresholds
plot.simruns(r.sel=rsel, thresh.lines=TRUE)

# select here whether to use pN.rdyn or pN.rstat projected abundance matrix
dim(pN.sel)              # 101 rows because first row is N0, final data year (year 0) which is starting point for simulation years 1-100
popsize <- t(pN.sel[-1, ])   # transpose to get 10,000 sim rows and 100 year columns (same format as Boyd's Bayesian code); remove that first row (YEAR 0)
dim(popsize)              # now in proper format: rows = # sims (10,000) and cols = # years in future (1-100)

# set up array of 1 matrix per threshold to store 1 if pop is above a threshold, 0 if at or below it
nsim <- nsims
tmax <- yrf
pop.aboveT <- array(1, c(nsim, tmax, 3))  # order is: rows=sims, cols=yrs, matrices: 3 threshold matrices w/ nrows=10000 sims, ncols=100 yrs)
dim(pop.aboveT)

# Need projections in terms of Projected Total Females in the population to compare to prescribed abundance thresholds of interest
# create empty matrix ProjTotFem that has rows = # sims (10,000) and cols = # years in future (1-100)
ProjTotFem <- matrix(NA, nrow=nsim, ncol=tmax)

# define start year for projection time series
if(data.type=="annual_Nests") sy <- remig       # if projections are of Annual Nests, start on future year 3 to compute run sum backward
if(data.type=="runsum_totalNests") sy <- 1  # if projections are of TOTAL Nests (already runsummed), start on future year 1
if(data.type=="runsum_totalFemales") sy <- 1 # if projections are of TOTAL Females (already runsummed), start on future year 1

# Fill in matrix of PROJECTED TOTAL FEMALES for each year to prepare for compare each to ABUNDANCE THRESHOLDS
for (i in 1:nsim) {
  for(tt in sy:tmax) {                   # start at year 3 since projections are of ANNUAL NESTS (not a run sum of total abund each year)
    if(data.type=="annual_Nests") 
      ProjTotFem[i,tt] <- sum(popsize[i,((tt-(remig-1)):tt)])/clutch.freq   # divide 3-yr Summed Annual Nests by clutch freq
    if(data.type=="runsum_totalNests") 
      ProjTotFem[i,tt] <- popsize[i,tt]/clutch.freq        # divide Total Nests (projected as such) by clutch freq
    if(data.type=="runsum_totalFemales") 
      ProjTotFem[i,tt] <- popsize[i,tt]    # no need to divide; already projected as TOTAL not Annual (no need to run sum)
  }
}
colMeans(ProjTotFem)  # view row 1 = sim 1 for all 100 years into future
plot(x=1:tmax, 
     y=colMeans(ProjTotFem, na.rm=TRUE), 
     ylab="Mean of Projections")
plot(x=1:tmax, 
     y=apply(ProjTotFem, MARGIN=2, FUN=quantile, probs=0.5, na.rm=TRUE), 
     ylab="Median of Projections")

# define abundance thresholds of interest: e.g., 50% of current abundance (total females in 2016 or sum annual nests or nesters from 2014-2016)
# Create an ABUNDANCE THRESHOLD matrix specific to each simulation run
# Each column is for a different abundance threshold (50% of current, 25%, or 12.5%)
ThreshTotFem <- matrix(NA, nrow=nsim, ncol=3)
ThreshTotFem[ ,1] <- 0.5*CurTotFem    # 50% thresh in column 1
ThreshTotFem[ ,2] <- 0.25*CurTotFem   # 25% thresh in column 2
ThreshTotFem[ ,3] <- 0.125*CurTotFem  # 12.5% thresh in column 3

# To Compare ProjTotFem to Specified Abundance thresholds (specific to each simulation run)
# go through each threshold "th", each sim "i", and projected year "y"
# leave as 1 if ProjTotFem is > ThreshTotFem and update to 0 if ProjTotFem <= ThreshTotFem
for(th in 1:3){
  for (i in 1:nsim) {
    for (y in 1:tmax) {
      if(!is.na(ProjTotFem[i, y]) & 
         ProjTotFem[i, y] <= ThreshTotFem[i, th]) {
        pop.aboveT[i,y,th] <- 0}   # row i = sim num, col y = fut year, th = thresh mat
    }
  }
}


```

Go through pop.aboveT output for each abundance threshold
Check if any sim runs *END* below the threshold (disregard runs in which pop dips below thresh but recovers to end above thresh) For sim runs that *END* below threshold, calculate the years until it drops and stays below thresh PIRO (A) & (B): Calculate mean, median, and 95% CI limits for "Years to Threshold" using all the sims that *END* below threshold PIRO request (C): estimate the probability of the pop reaching those thresholds (50%, 25%, 12.5% of current abundance) in 5, 10, 25, 50, and 100 year time intervals with associated 95% confidence intervals


```{r}
  TIMEtoTHRESH <- data.frame(matrix(nrow=3, ncol=6), 
                             row.names=c("50% abund", "25% abund", "12.5% abund"))

  names(TIMEtoTHRESH) <- c("probEndAbove", "probEndBelow", 
                           "MeanYrsToThresh", "2.5%", "50%", "97.5%")
  TIMEtoTHRESH

  eval.yrs <- c(5,10,25,50,100)   # year in future at which to evaluate prob of reaching the abundance thresholds
  PROBatYEAR <- data.frame(matrix(nrow=3, ncol=15), 
                           row.names=c("50% abund", "25% abund", "12.5% abund"))
  
  names(PROBatYEAR) <- c("Y5", "Y5l", "Y5u", "Y10", "Y10l", "Y10u", "Y25", "Y25l", "Y25u",
                         "Y50", "Y50l", "Y50u", "Y100", "Y100l", "Y100u")
  PROBatYEAR

# THRESHOLDS: cycle through
  for (th in 1:3){
    # for a threshold matrix (1= sim/yr projection above curr abund thresh %; 0= at or below thresh)
    # for all runs ending in 1, consider them a success -- pop did not fall/stay below the thresh
      simsSums <- rowSums(pop.aboveT[,,th])          # sum each row (sim); if a run stayed above thresh whole time, sum = 100
      simsDropBelow <- which(simsSums<tmax)          # identify rows that may DROP below thresh at some point (but may still recover and end up ABOVE thresh)
      simsEndBelow <- which(pop.aboveT[,tmax,th]!=1) # identify sim runs (rows) that DON'T end in 1; ended at tmax below thresh
      simsEndAbove <- which(pop.aboveT[,tmax,th]==1) # identify sim runs (rows) that DO end in 1; ended at tmax below thresh
      simsDropBelowEndAbove <- which(simsSums<tmax & pop.aboveT[,tmax,th]==1)  # sim runs that drop below thresh but then recover to end at tmax above thresh

    # if no sim runs end below thresh, then prob of reaching thresh within 100 yrs is 0 and "Years to reach thresh" is NA
      if(length(simsEndBelow)==0) {
        probEndBelow <- 0 
        probEndAbove <- 1 
        YrsToThresh <- NA 
        YrsToThresh.mean <- NA 
        YrsToThresh.quants <- c(NA, NA, NA)
      }
      
      # if some sim runs end below thresh, then look to see when they fall below and stay below (max year when pop.aboveT = 1 for that sim run)
      if(length(simsEndBelow)!=0) {
        probEndBelow <- length(simsEndBelow)/nsim
        probEndAbove <- length(simsEndAbove)/nsim
        YrsToThresh <- apply(X=pop.aboveT[simsEndBelow, , th], 
                             MARGIN=1, 
                             FUN=function(x){max(which(x==1))})
        YrsToThresh.mean <- mean(YrsToThresh)
        YrsToThresh.quants <- quantile(YrsToThresh, 
                                       probs=c(0.025, 0.50, 0.975))
      }
      
      # Package the output for each threshold
      out <- c(probEndAbove, 
               probEndBelow, 
               round(YrsToThresh.mean, 1), 
               round(YrsToThresh.quants,1))
      
      out.df <- as.data.frame(t(out))
      names(out.df) <- c("probEndAbove", "probEndBelow", "MeanYrsToThresh", "2.5%", "50%", "97.5%")
      TIMEtoTHRESH[th,] <- out.df

    # Prob of reach the threshold at year 5, 10, 25, 50, 100
      pop.aboveT.edit <- pop.aboveT
      pop.aboveT.edit[simsDropBelowEndAbove, , th] <- 1    # change 0s to 1s for sim runs that dropped below thresh but recovered to end above

    # EVALUATION YEARS: cycle through
      for (ey in 1:length(eval.yrs)) {
        fut.yr <- eval.yrs[ey]
        simsReach <- which(pop.aboveT.edit[ , fut.yr, th]==0)  # to get prob of reaching thresh in that fut.yr (don't later recover), calc nmbr sim rows=0
        PROBatYEAR[th, 1+(ey-1)*3] <- length(simsReach)/nsim

      # Bootstrap 95% CI from the nsim results for the YEAR and THRESH
          CI = 0.95
          nboot = nsim       # set bootstrap number of runs = nsim
          x = as.vector(pop.aboveT.edit[ , fut.yr, th])    # col of specified YEAR for the threshold;
          n = length(x)                                    # should be same as nsims
          xStat = length(which(x == 0))     # calculate the statistic on original sample: Prob of runs falling below an abund threshold
        # Generate 'nboot' number of bootstrap samples, i.e. an n x nboot array of  (rows=n ; cols=nboots)
          tmpdata = sample(x=x,size=n*nboot, replace=TRUE)         # random resamples from x
          bootstrapsample = matrix(tmpdata, nrow=n, ncol=nboot)
        # Compute the Probability stat for each resample (column of bootstrapsample)
          bsStats =  apply(X=bootstrapsample, 
                           MARGIN=2, 
                           FUN=function(simvec){length(which(simvec == 0))}) # MARGIN=2 for columns of matrix
          bsStats.mean = mean(bsStats)   # mean of the bootstrapped sample statistics = the sample statistic calculated on the original sample
        # Compute delta* for each bootstrap sample [delta* = resample stat - original sample stat (xbar)]
          deltastar = bsStats - xStat    # (integer) diff in number of runs falling at/below threshold between bootstrap sample and original sample
        # Find the Lower and Upper CI quantile for deltastar
          ciU = 1-(1-CI)/2      # lower/upper confidence interval limits e.g. 0.025 for 2.5% & 0.975 for 97.5% for a 95% CI
          ciL = 0+(1-CI)/2
          d = quantile(deltastar, c(ciL, ciU))  #  the quantile() function is sophisticated about choosing a quantile between two data points.
        # Calculate the specified confidence interval for original sample stat.
          ci = xStat - c(d[2], d[1])
          ci.prob <- ci/nsim
        # Add Bootstrapped CI to the storage matrix for this threshold row in appropriate year cols
          PROBatYEAR[th, (2+(ey-1)*3): (3+(ey-1)*3)] <- ci.prob

        } # END CYCLE THROUGH EVALUTION YEARS
  }  # END CYCLE THROUGH THRESHOLDS

  TIMEtoTHRESH  # Answers PIRO request (A) and (B)
  PROBatYEAR    # Answers PIRO request (C)

  rmean         # Answers PIRO request  (D) population's mean log growth rate
  rvar          # Answers PIRO request  (D) variance of mean log growth rate
  rci           # Answers PIRO request  (D) 95% confidence interval

  lambda.mean   # Answers PIRO request  (E) finite rate of increase, lambda
  lambda.ci     # Answers PIRO request  (D) 95% confidence interval for lambda


# Start writing to an output file
  sink('analysis-output.txt')
  print("(A & B) -------------- ")
  print("TIMEtoTHRESH")
  TIMEtoTHRESH  # Answers PIRO request (A) and (B)
  cat("\n\n")

  print("(C) -------------- ")
  print("PROBatYEAR")
  round(PROBatYEAR,3)    # Answers PIRO request (C)
  cat("\n\n")

  print("(D) -------------- ")
  print("Population's mean log growth rate, variance, and 95% confidence interval")
  as.numeric(rmean)         # Answers PIRO request  (D) population's mean log growth rate
  rvar          # Answers PIRO request  (D) variance of mean log growth rate
  rci           # Answers PIRO request  (D) 95% confidence interval
  cat("\n\n")

  print("(E) -------------- ")
  print("Population's finite rate of increase (lambda) and 95% confidence interval")
  as.numeric(lambda.mean)   # Answers PIRO request  (E) finite rate of increase, lambda
  lambda.ci     # Answers PIRO request  (E) 95% confidence interval for lambda
  # Stop writing to the file
  sink()


```

